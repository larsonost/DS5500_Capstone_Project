Many large databases with an abundance of image data suffer from the lack of sufficient labeling, resulting in the inability to optimally search and index its contents. For this project, we plan to develop an application capable of digesting an image and returning a descriptive textual caption to accurately label the image’s contents. Through this solution, we will significantly improve the ability for images in a database to gain visibility through text-based search, the ability to search a database with an image, and enhance the relevancy of results by providing more accurate and relevant searches. This application can be particularly beneficial for eCommerce platforms, where the volume of images is immense, and proper labeling is crucial for efficient product discovery and customer satisfaction.

The model will employ an encoder-decoder architecture. We will evaluate multiple transfer learning models for the encoder including common CNN and transformer models. The decoder will be an LSTM or transformer, facilitating the visual input into descriptive textual output. We will leverage the MS COCO Captions Dataset, which contains over one and a half million captions describing over 330,000 images [2]. The dataset’s images consist of many common items, animals, people, and vehicles in different situations with varying perspectives.
