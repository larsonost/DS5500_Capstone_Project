{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This file uses the CNN + LSTM architecture to label images from the coco captions dataset.\n",
        "# CITATION: The LSTM model and its embeddings are insprired from the following tutorial: https://www.analyticsvidhya.com/blog/2020/11/create-your-own-image-caption-generator-using-keras/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlwkqQFtCEnk"
      },
      "outputs": [],
      "source": [
        "# Basic Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import pickle\n",
        "import json\n",
        "import random\n",
        "import collections\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Auixillary\n",
        "from google.colab import drive\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.eval import COCOEvalCap\n",
        "\n",
        "# Tensorflow & Keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from keras import Input\n",
        "\n",
        "# Commands\n",
        "warnings.filterwarnings('ignore')\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create the local directories\n",
        "#!unzip /content/drive/MyDrive/glove.6B.200d.txt -d /content/glove6b\n",
        "#!unzip /content/drive/MyDrive/coco2017.zip -d /content/coco2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yXwYueyCLsK"
      },
      "outputs": [],
      "source": [
        "# Initializing COCO API with the annotations file\n",
        "coco = COCO('/content/coco2017/annotations/captions_train2017.json')\n",
        "\n",
        "# Loading annotations from JSON into a variable\n",
        "with open('/content/coco2017/annotations/captions_train2017.json', 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Creating a mapping of image paths to their captions\n",
        "image_caption_map = collections.defaultdict(list)\n",
        "\n",
        "# Populating the image-caption mapping\n",
        "for val in annotations['annotations']:\n",
        "  im_path = '/content/coco2017/train2017/' + '%012d.jpg' % (val['image_id'])\n",
        "  image_caption_map[im_path].append((f\"{val['caption']}\"))\n",
        "\n",
        "# Reducing the dataset size by sampling a subset\n",
        "image_caption_map = dict(random.sample(dict(image_caption_map).items(),\n",
        "                              int(len(dict(image_caption_map)) * 0.001)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DcRQSbOwCUIF"
      },
      "outputs": [],
      "source": [
        "# Extract image ID from its path\n",
        "def get_image_id(im_path):\n",
        "    return im_path.split('/')[-1].rsplit('.', 1)[0]\n",
        "\n",
        "# Remove punctuation and convert description to lowercase\n",
        "def process_description(desc):\n",
        "    return ''.join(ch for ch in desc if ch not in set(string.punctuation)).lower()\n",
        "\n",
        "# Convert image-caption mapping to ID-caption format\n",
        "def transform_to_id_caption_format(image_caption_map):\n",
        "    new_data = {}\n",
        "    for path, caps in image_caption_map.items():\n",
        "        new_data[get_image_id(path)] = [process_description(cap) for cap in caps]\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QVo3aWliCUKc"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the captions by removing punctuations and converting to lowercase\n",
        "preprocessed = {}\n",
        "for path, caps in image_caption_map.items():\n",
        "    preprocessed[path] = [process_description(cap) for cap in caps]\n",
        "\n",
        "# Transforming preprocessed captions into a new format based on image IDs\n",
        "new_image_map = transform_to_id_caption_format(preprocessed)\n",
        "\n",
        "# Creating lines of text with image paths and their processed descriptions\n",
        "lines = [f\"{path} {description}\"\n",
        "         for path, d_list in new_image_map.items()\n",
        "         for description in d_list]\n",
        "\n",
        "# Joining all lines into a single string\n",
        "new_descriptions = '\\n'.join(lines)\n",
        "\n",
        "# Extracting image paths from the map for training\n",
        "train_img = list(image_caption_map.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjU5GoUqCUMu"
      },
      "outputs": [],
      "source": [
        "# Initializing a dictionary to hold batch of processed descriptions\n",
        "descriptions_batch = {}\n",
        "lines = new_descriptions.split('\\n')\n",
        "\n",
        "# Processing each line in the new descriptions\n",
        "for line in tqdm(lines):\n",
        "    if not line.strip():\n",
        "        continue\n",
        "    img_id, *img_desc = line.split()\n",
        "    # Adding 'startseq' and 'endseq' tokens to each description\n",
        "    if img_id in list(new_image_map.keys()):\n",
        "        desc = 'startseq {} endseq'.format(' '.join(img_desc))\n",
        "        descriptions_batch.setdefault(img_id, []).append(desc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o8JJhx3SCUPB"
      },
      "outputs": [],
      "source": [
        "# Creating a flat list of all captions\n",
        "captions_batch = list(itertools.chain.from_iterable(descriptions_batch.values()))\n",
        "\n",
        "# Generating a vocabulary from the captions\n",
        "words = [word for sent in captions_batch for word in sent.split()]\n",
        "vocab = [word for word, count in Counter(words).items() if count >= 10]\n",
        "\n",
        "# Creating word-to-index and index-to-word mappings\n",
        "wordtoix = {word: index+1 for index, word in enumerate(vocab)}\n",
        "ixtoword = {index: word for word, index in wordtoix.items()}\n",
        "vocab_size = len(ixtoword) + 1\n",
        "\n",
        "# Gathering all descriptions to determine the max length\n",
        "all_desc = [desc for descs in descriptions_batch.values() for desc in descs]\n",
        "max_length = max(map(lambda x: len(x.split()), all_desc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W3aMMPnCCURZ"
      },
      "outputs": [],
      "source": [
        "# Initializing a dictionary to store word embeddings\n",
        "embedding_map = {}\n",
        "with open('/content/glove6b/glove.6B.200d.txt', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        embedding_map[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# Defining the embedding dimension\n",
        "embedding_dim = 200\n",
        "embedding_mat = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Populating the embedding matrix with known embeddings\n",
        "embeddings_keep = {wordtoix[word]: embedding_map[word] for word in wordtoix if word in embedding_map}\n",
        "for index, embeds in embeddings_keep.items():\n",
        "    embedding_mat[index] = embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU7WuudlCUTn"
      },
      "outputs": [],
      "source": [
        "# Setting up the InceptionV3 model for image feature extraction\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "model_new = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "\n",
        "# Function to preprocess input images for the model\n",
        "def image_preprocessing(im_path):\n",
        "    image = load_img(im_path, target_size=(299, 299))\n",
        "    image_arr = img_to_array(image)\n",
        "    image_arr = np.expand_dims(image_arr, axis=0)\n",
        "    return preprocess_input(image_arr)\n",
        "\n",
        "# Extracting feature vectors from preprocessed images\n",
        "def fetch_encoding(im_path):\n",
        "    processed_image = image_preprocessing(im_path)\n",
        "    feature_vector = model_new.predict(processed_image)\n",
        "    return np.squeeze(feature_vector)\n",
        "\n",
        "# Processing all training images to get their feature vectors\n",
        "training_data = {im_path.split('/')[-1]: fetch_encoding(im_path) for im_path in tqdm(train_img)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uHqexrs8CkWZ"
      },
      "outputs": [],
      "source": [
        "# Defining the path to save the training data\n",
        "save_path = '/content/drive/MyDrive/training_data_1027231301.pkl'\n",
        "\n",
        "# Saving the training data to a file using pickle\n",
        "with open(save_path, 'wb') as file:\n",
        "    pickle.dump(training_data, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Loading the training data back from the file\n",
        "with open(save_path, 'rb') as file:\n",
        "    training_data_loaded = pickle.load(file)\n",
        "\n",
        "# Assigning the loaded data to the original variable\n",
        "training_data = training_data_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U2KpWv1CkYy"
      },
      "outputs": [],
      "source": [
        "# Create initial feature layer\n",
        "def feature_extractor(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Dropout(0.5)(inputs)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    return inputs, x\n",
        "\n",
        "# Create sequence generator\n",
        "def sequence_processor(input_shape, vocab_size, embedding_dim, embedding_mat):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Embedding(vocab_size, embedding_dim, mask_zero=True, weights=[embedding_mat], trainable=False)(inputs)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = LSTM(256)(x)\n",
        "    return inputs, x\n",
        "\n",
        "# Combine the entire model\n",
        "def model_assembler(feature_output, sequence_output, vocab_size):\n",
        "    x = add([feature_output, sequence_output])\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    return Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "# Funnel the features through the built model\n",
        "feature_input, feature_output = feature_extractor((2048,))\n",
        "sequence_input, sequence_output = sequence_processor((max_length,), vocab_size, embedding_dim, embedding_mat)\n",
        "outputs = model_assembler(feature_output, sequence_output, vocab_size)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[feature_input, sequence_input], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aqNF4YUACkbI"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess a given description into sequences for training\n",
        "def preprocess_description(desc, wordtoix, max_length):\n",
        "    seq = [wordtoix.get(word) for word in desc.split() if word in wordtoix]\n",
        "    seq = [s for s in seq if s is not None]\n",
        "    X2, y = [], []\n",
        "\n",
        "    # Create input-output pairs from the sequence\n",
        "    for i in range(1, len(seq)):\n",
        "        in_seq, out_seq = seq[:i], seq[i]\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "    return X2, y\n",
        "\n",
        "# Generator function to yield batches of data for training\n",
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = [], [], []\n",
        "    while True:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            photo = photos.get(key + '.jpg')\n",
        "            if photo is None:\n",
        "                continue\n",
        "\n",
        "            # Process each description for the photo\n",
        "            for desc in desc_list:\n",
        "                in_seqs, out_seqs = preprocess_description(desc, wordtoix, max_length)\n",
        "                X1.extend([photo] * len(in_seqs))\n",
        "                X2.extend(in_seqs)\n",
        "                y.extend(out_seqs)\n",
        "\n",
        "            # Yield a batch of data when the desired batch size is reached\n",
        "            if len(X1) >= num_photos_per_batch:\n",
        "                yield ([np.array(X1), np.array(X2)], np.array(y))\n",
        "                X1, X2, y = [], [], []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoRTjzzqCu3y"
      },
      "outputs": [],
      "source": [
        "# Set training hyperparams\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "steps = len(descriptions_batch) // batch_size\n",
        "generator = data_generator(descriptions_batch, training_data, wordtoix, max_length, batch_size)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF8Oy33NCu6L"
      },
      "outputs": [],
      "source": [
        "# Generate a caption given a path\n",
        "def generate_caption(photo, max_len=max_length):\n",
        "    caption = ['startseq']\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tokens = [wordtoix[word] for word in caption if word in wordtoix]\n",
        "        tokens_padded = pad_sequences([tokens], maxlen=max_len)\n",
        "        predicted_word_index = np.argmax(model.predict([photo, tokens_padded], verbose=0))\n",
        "        predicted_word = ixtoword[predicted_word_index]\n",
        "        if predicted_word == 'endseq':\n",
        "            break\n",
        "        caption.append(predicted_word)\n",
        "    processed_caption = caption[1:]\n",
        "    if 'endseq' in processed_caption:\n",
        "        processed_caption.remove('endseq')\n",
        "    return ' '.join(processed_caption)\n",
        "\n",
        "# Get a caption given a path\n",
        "def get_caption(im_path):\n",
        "    image = fetch_encoding(im_path)\n",
        "    image = image.reshape((1, 2048))\n",
        "    x=plt.imread(im_path)\n",
        "    plt.imshow(x)\n",
        "    plt.show()\n",
        "    print(generate_caption(image))\n",
        "    return generate_caption(image)\n",
        "\n",
        "# Get example image\n",
        "get_caption('/content/coco2017/train2017/000000266677.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCr4ow9CCu8M"
      },
      "outputs": [],
      "source": [
        "# Defining the directory path for validation images\n",
        "folder_path = '/content/coco2017/val2017/'\n",
        "image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "# Constructing the full path for each image file\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "\n",
        "# List to store generated captions for each image\n",
        "all_captions = []\n",
        "\n",
        "# Processing each image to generate captions\n",
        "for image_file in tqdm(image_files):\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "    temp_dict = dict()\n",
        "\n",
        "    # Extracting image ID and encoding from image path\n",
        "    image_id = int(str(image_path).split('.')[0].split('/')[-1].lstrip('0'))\n",
        "    image = fetch_encoding(image_path)\n",
        "    image = image.reshape((1, 2048))\n",
        "\n",
        "    # Generating caption for the image\n",
        "    image_caption = generate_caption(image)\n",
        "    temp_dict[\"image_id\"] = image_id\n",
        "    temp_dict[\"caption\"] = image_caption\n",
        "    all_captions.append(temp_dict)\n",
        "\n",
        "# Saving all generated captions to a JSON file\n",
        "try:\n",
        "    with open('/content/results.json', 'w') as f:\n",
        "        json.dump(all_captions, f)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Defining paths for annotation and results files\n",
        "annotation_file = '/content/coco2017/annotations/captions_val2017.json'\n",
        "results_file = '/content/results.json'\n",
        "\n",
        "# Evaluating generated captions against ground truth using COCO tools\n",
        "coco = COCO(annotation_file)\n",
        "coco_result = coco.loadRes(results_file)\n",
        "coco_eval = COCOEvalCap(coco, coco_result)\n",
        "coco_eval.evaluate()\n",
        "\n",
        "# Displaying evaluation scores for various metrics\n",
        "for metric, score in coco_eval.eval.items():\n",
        "    print(f'{metric}: {score:.3f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
